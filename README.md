
# Aprendizaje por Refuerzo 1 
## GridWorld 4√ó4 (Q-Learning, SARSA, MC-ES, MC-IS) + Snake (DQN + CNN)

**Autora:** Noelia Melina Qualindi  
**Materia:** Aprendizaje por Refuerzo I ‚Äî Maestr√≠a en IA (UBA)  
**Docente:** Miguel Augusto Azar  
**A√±o:** 2025

Este repositorio contiene:
- Un entorno **GridWorld 4√ó4** (tipo *FrozenLake*, `slip=0.2`) con **Q-Learning**, **SARSA**, **Monte Carlo ES**, y **Monte Carlo IS** (Ordinary & Weighted).  
- Un juego **Snake** con observaciones tipo imagen y un **DQN con CNN (PyTorch)**, con **app Streamlit** para jugar **manual**, hacer **inferencia** con el modelo (juega solo) y **entrenar** en vivo.  
- Notebook de **reproducibilidad** para entrenar DQN en Snake sin Streamlit.

---

## üöÄ Quick Start ‚Äî GridWorld

```bash
# 1) (Opcional) crear y activar venv
python -m venv .venv
# Windows (PowerShell)
.venv\Scripts\Activate.ps1
# macOS / Linux
source .venv/bin/activate

# 2) instalar dependencias
pip install -r requirements.txt
# o m√≠nimas (solo GridWorld):
# pip install streamlit matplotlib numpy

# 3) correr la app GridWorld
streamlit run app_gridworld.py
# si streamlit no se reconoce:
# python -m streamlit run app_gridworld.py
```

La terminal mostrar√° algo como:
```
Local URL: http://localhost:8501
```
Abr√≠ esa URL en el navegador.

---

## üïπÔ∏è Uso de la app GridWorld

1. En la **sidebar**:
   - Ajust√° `slip_prob`, `max_steps` y `seed` del entorno.
   - Eleg√≠ el **algoritmo** (Q-Learning, SARSA, MC-ES, MC-IS Weighted/Ordinary).
   - Configur√° **hiperpar√°metros**:
     - Q-Learning / SARSA: `Œ±`, `Œ≥`, `Œµ inicial`, `Œµ decay`, `Œµ m√≠nimo`.
     - MC-* : `Œ≥` y `Œµ_b` (pol√≠tica de comportamiento ~ uniforme).
2. Puls√° **‚ÄúEntrenar y visualizar‚Äù**.
3. Se mostrar√°n dos gr√°ficos:
   - **Pol√≠tica greedy derivada (Œµ=0)** con marcadores `> v < ^` y etiquetas S/G/X.
   - **Convergencia** (reward por episodio, **media m√≥vil=100**).
4. Debajo, se listan **m√©tricas** (evaluaci√≥n greedy, 2000 episodios):  
   **tasa de √©xito**, **pasos promedio (√©xitos)** y **pasos promedio (todos)**.
5. Secci√≥n **‚ÄúAnimaci√≥n del episodio‚Äù**:
   - Eleg√≠ **FPS**, **Œµ de animaci√≥n** y **semilla**.
   - Puls√° **‚ÄúAnimar episodio‚Äù** para ver el recorrido del agente con *slip*.

---

## ‚öôÔ∏è Par√°metros por defecto ‚Äî GridWorld (demo)

- `n_episodes = 4000`, `max_steps = 100`, `slip_prob = 0.2`, `seed = 42`
- Q-Learning / SARSA: `Œ± = 0.8`, `Œ≥ = 0.95`, `Œµ0 = 1.0`, `Œµ_decay = 0.999`, `Œµ_min = 0.01`
- MC-ES: *Exploring Starts*
- MC-IS (Ordinary/Weighted): \(b(a\mid s)\approx\) uniforme (`Œµ_b = 1.0`), con **cap** \(œÅ, W \le 10^8\).

---

## üß™ Protocolo de evaluaci√≥n ‚Äî GridWorld

- Evaluaci√≥n **greedy (Œµ=0)**, **2000** episodios, `max_steps=100`.
- M√©tricas: **tasa de √©xito**, **pasos promedio (√©xitos)** y **pasos promedio (todos)**.
- Recomendado fijar `seed=42` para comparabilidad.

---

## üêç Snake (DQN + CNN + Streamlit)

**Qu√© es:** un entorno **Snake** propio (sin Gymnasium) con observaci√≥n tipo imagen \((3,H,W)\): **cabeza**, **cuerpo**, **comida**.  
Implementa un **DQN** con **CNN** (PyTorch), **Replay Buffer**, **Target Network**, **Œµ-decay** y **p√©rdida Huber**.  
La app Streamlit permite **jugar manualmente**, **entrenar** y **hacer inferencia** (auto-play) con el modelo.

### Quick Start ‚Äî Snake

```bash
# (con el mismo venv)
# dependencias m√≠nimas para Snake
pip install torch matplotlib numpy streamlit

# correr la app Snake (eleg√≠ el archivo que tengas)
streamlit run app_snake_streamlit.py
# o
# streamlit run app_snake.py
```

**Device:** usa autom√°ticamente **CUDA** (si hay GPU), sino **MPS** (Apple Silicon) y, si no, **CPU**.  
La red usa **Global Average Pool (1√ó1)** ‚Üí compatible con MPS (evita el error ‚ÄúAdaptive pool MPS: input sizes must be divisible‚Ä¶‚Äù).

### Modos de la app Snake

- **Manual**: jug√°s vos con los botones (‚Üë ‚Üì ‚Üê ‚Üí).  
  - Cuando el episodio termina, los botones se **deshabilitan**; presion√° **Reset entorno** para reiniciar.
- **Entrenar DQN**: ajust√°s hiperpar√°metros y entren√°s en vivo.  
  - Se muestra **convergencia** (Reward por episodio, **MA=50**).  
  - Al finalizar, los pesos del modelo quedan en memoria de la app.
- **Inferencia DQN**: el agente juega solo con el modelo entrenado.  
  - Pod√©s elegir **FPS** y un **Œµ de inferencia** para Œµ-greedy.

### Hiperpar√°metros recomendados ‚Äî Snake
- Grid \(H=W\) = 12 (slider 6‚Äì20), `max_steps=300`, `episodes=600‚Äì1000`, `seed=42`
- DQN: `Œ≥=0.99`, `lr=1e-3` (Adam), `batch_size=64`, `buffer_size=50_000`  
  `start_learn=1_000`, `target_update_freq=1_000`, `Œµ0=1.0`, `Œµ_min=0.05`, `Œµ_decay=0.995`

### Tips de UI ‚Äî Snake
- **Tama√±o del tablero** en pantalla: control por `figsize` (XS/S/M/L) para que no se vea gigante.  
- **Persistencia**: se usa `st.session_state` para pesos, returns, `done`, etc.  
- **Botones seguros**: `do_step` ignora clicks cuando `done=True` (evita `RuntimeError`).

---

## üìì Reproducibilidad (sin Streamlit)

Incluyo una **notebook** autocontenida para entrenar y evaluar el DQN de Snake en modo script:

- **Archivo:** `Snake_DQN_Notebook.ipynb`  
- **Instalar:** `pip install torch matplotlib numpy`  
- **Contenido:** entorno `SnakeEnv`, red `DQNCNN` con **GlobalAvgPool**, **Replay Buffer**, **Target Network**,  
  entrenamiento configurable, **gr√°fico de convergencia (MA=50)**, evaluaci√≥n **greedy** y guardado de pesos `.pth`.

> Si prefer√≠s script CLI: `python dqn_snake.py` (incluido si trabaj√°s con los archivos de ejemplo).

---

## üì¶ Estructura sugerida

```
.
‚îú‚îÄ app_gridworld.py                 # App Streamlit (GridWorld: entrenar, pol√≠tica, convergencia, animaci√≥n)
‚îú‚îÄ app_snake_streamlit.py          # App Streamlit (Snake: Manual, Entrenar DQN, Inferencia DQN)
‚îú‚îÄ README.md                       # Este documento
‚îú‚îÄ requirements.txt                # Dependencias m√≠nimas (ver m√°s abajo)
‚îú‚îÄ rl_figs/                        # (se crea) PNGs de pol√≠ticas / convergencia
‚îî‚îÄ notebooks/                      # notebooks de apoyo
```
---

## üìã requirements.txt (m√≠nimo recomendado)

```txt
streamlit
matplotlib
numpy
torch
```

> Para solo GridWorld: `streamlit matplotlib numpy`. Para Snake (DQN): agregar `torch`.

---

## üõ†Ô∏è Troubleshooting (GridWorld + Snake)

- **Streamlit no abre el navegador:** copi√° manualmente `http://localhost:8501`.  
- **Puerto en uso:** `streamlit run <app.py> --server.port 8502`.  
- **`streamlit` no se reconoce:** `python -m streamlit run <app.py>`.  
- **MPS (Apple Silicon) y Adaptive Pool:** usar **GlobalAvgPool 1√ó1** (ya incluido) para evitar el error de divisibilidad.  
- **Episodio terminado en modo manual (Snake):** los botones se deshabilitan; si insist√≠s, `do_step` lo ignora y te pide **Reset entorno**.  
- **Dispositivo:** se elige autom√°ticamente entre **CUDA ‚Üí MPS ‚Üí CPU**; si algo no est√° soportado en MPS, forz√° `device=cpu`.

---

## ‚úçÔ∏è Notas finales

- En GridWorld, las pol√≠ticas mostradas son **greedy (Œµ=0)** derivadas de \(Q\) tras el entrenamiento.  
- **SARSA** entrena con **on-policy Œµ-greedy**; su visualizaci√≥n greedy es solo para comparaci√≥n.  
- En Snake, la **convergencia (MA=50)** muestra la mejora del DQN con CNN sobre observaciones pixeladas.  
- La **reproducibilidad** se garantiza con semillas y la **notebook** adjunta.
