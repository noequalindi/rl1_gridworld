
# RL Challenge ‚Äî GridWorld 4√ó4 (Q-Learning, SARSA, MC-ES, MC-IS)

**Autora:** Noelia Melina Qualindi  
**Materia:** Aprendizaje por Refuerzo I ‚Äî Maestr√≠a en IA (UBA)  
**Docente:** Miguel Augusto Azar  
**A√±o:** 2025

Este repositorio contiene un entorno **GridWorld 4√ó4** con din√°mica tipo *FrozenLake* (slip=0.2)
y la implementaci√≥n de los algoritmos **Q-Learning**, **SARSA**, **Monte Carlo ES**, y **Monte Carlo IS**
(en variantes **Ordinary** y **Weighted**). Incluye una **app Streamlit** para entrenar en vivo,
visualizar **convergencia**, **pol√≠ticas greedy** y **animar** episodios.

---

## üöÄ Quick Start

```bash
# 1) (Opcional) crear y activar venv
python -m venv .venv
# Windows (PowerShell)
.venv\Scripts\Activate.ps1
# macOS / Linux
source .venv/bin/activate

# 2) instalar dependencias
pip install -r requirements.txt
# o m√≠nimas:
# pip install streamlit matplotlib numpy

# 3) correr la app
streamlit run app_gridworld.py
# si streamlit no se reconoce:
# python -m streamlit run app_gridworld.py
```

La terminal mostrar√° algo como:
```
Local URL: http://localhost:8501
```
Abr√≠ esa URL en tu navegador.

---

## üì¶ Estructura sugerida

```
.
‚îú‚îÄ app_gridworld.py                # App Streamlit (entrenar, ver pol√≠tica, animar episodios)
‚îú‚îÄ README.md                       # Este documento
‚îú‚îÄ requirements.txt                # Dependencias m√≠nimas
‚îú‚îÄ rl_figs/                        # (se crea) PNGs de pol√≠ticas / convergencia, si us√°s scripts que guardan
‚îî‚îÄ notebooks/                      # (opcional) notebooks de apoyo
```

> Si est√°s trabajando con un bundle/zip, basta con tener `app_gridworld.py` y `requirements.txt` para correr la app.

---

## üß† Algoritmos incluidos

- **Q-Learning** (off-policy TD)  
- **SARSA** (on-policy TD, entrenamiento con pol√≠tica Œµ-greedy)  
- **MC-ES** (*Exploring Starts*, control first-visit)  
- **MC-IS (Ordinary)** (off-policy con importancia **ordinaria**)  
- **MC-IS (Weighted)** (off-policy con importancia **ponderada**, menor varianza)

**Importante:** Las figuras de pol√≠tica que se muestran son **greedy derivadas** a partir de \(Q\) con **Œµ=0** (para comparar algoritmos).
Durante el **entrenamiento**, Q-Learning y SARSA usan **Œµ-greedy** para explorar.

---

## üïπÔ∏è Uso de la app

1. En la **sidebar**:
   - Ajust√° `slip_prob`, `max_steps` y `seed` del entorno.
   - Eleg√≠ el **algoritmo** a entrenar.
   - Configur√° **hiperpar√°metros**:
     - Q-Learning / SARSA: `Œ±`, `Œ≥`, `Œµ inicial`, `Œµ decay`, `Œµ m√≠nimo`.
     - MC-* : `Œ≥` y `Œµ_b` (pol√≠tica de comportamiento ~ uniforme).
2. Puls√° **‚ÄúEntrenar y visualizar‚Äù**.
3. Se mostrar√°n dos gr√°ficos:
   - **Pol√≠tica greedy derivada (Œµ=0)** con marcadores `> v < ^`.
   - **Convergencia** (reward por episodio, **media m√≥vil=100**).
4. Debajo, se listan **m√©tricas** (evaluaci√≥n greedy, 2000 episodios):  
   **tasa de √©xito**, **pasos promedio (√©xitos)** y **pasos promedio (todos)**.
5. Secci√≥n **‚ÄúAnimaci√≥n del episodio‚Äù**:
   - Eleg√≠ **FPS**, **Œµ de animaci√≥n** (si quer√©s Œµ-greedy) y **semilla**.
   - Puls√° **‚ÄúAnimar episodio‚Äù** para ver el recorrido del agente con *slip* activo.

---

## ‚öôÔ∏è Par√°metros por defecto (recomendados para demo)

- `n_episodes = 4000`, `max_steps = 100`, `slip_prob = 0.2`, `seed = 42`
- Para Q-Learning y SARSA: `Œ± = 0.8`, `Œ≥ = 0.95`, `Œµ0 = 1.0`, `Œµ_decay = 0.999`, `Œµ_min = 0.01`
- MC-ES: *Exploring Starts*
- MC-IS (Ordinary/Weighted): \(b(a\mid s)\approx\) uniforme (`Œµ_b = 1.0`), con **cap** de seguridad para pesos \(œÅ, W \le 10^8\).

---

## üß™ Protocolo de evaluaci√≥n (app)

- Evaluaci√≥n **greedy (Œµ=0)**, **2000** episodios, corte a **100** pasos por episodio.
- M√©tricas:
  - **Tasa de √©xito** (fracci√≥n de episodios que alcanzan la meta).
  - **Pasos promedio (√©xitos)**: pasos promediados **solo** sobre episodios exitosos.
  - **Pasos promedio (todos)**: promedio global.

---

## üîÅ Reproducibilidad

- El entorno y los entrenamientos usan `seed` para `numpy` y `random`.
- Recomendado mantener `seed=42` para resultados consistentes en presentaciones.

---

## üõ†Ô∏è Troubleshooting

- **No abre el navegador:** copi√° manualmente `http://localhost:8501` en el navegador.
- **Puerto en uso:** `streamlit run app_gridworld.py --server.port 8502`.
- **Firewall/Proxy:** eleg√≠ un puerto permitido (p. ej. 8080).
- **`streamlit` no se reconoce:** `python -m streamlit run app_gridworld.py`.
- **NameError (`policy`/`rewards`):** la app usa `st.session_state` para persistir el resultado del entrenamiento entre recargas. Si modific√°s el c√≥digo, asegurate de mantener esa secci√≥n.

---

## üìã requirements.txt (m√≠nimo)

```txt
streamlit
matplotlib
numpy
```

> Si us√°s notebooks con exportaci√≥n a `.docx` u otras utilidades, agreg√° paquetes extra seg√∫n corresponda.

---

## ‚úçÔ∏è Notas finales

- Las pol√≠ticas mostradas son **greedy (Œµ=0)** derivadas de \(Q\) tras el entrenamiento.
- **SARSA** entrena con **on-policy Œµ-greedy**; por eso, su visualizaci√≥n greedy es solo para comparaci√≥n.
- **MC-IS Ordinary** puede presentar **alta varianza**; la variante **Weighted** es m√°s estable en pr√°ctica.
